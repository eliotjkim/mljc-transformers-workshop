{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51a8c368",
   "metadata": {},
   "source": [
    "# Ozone Transformer Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cdf3a9",
   "metadata": {},
   "source": [
    "## Environments Setup\n",
    "\n",
    "1. Create Python virtual environment named `venv`\n",
    "   - `python -m venv venv`\n",
    "2. Activate the new virtual environment `venv`\n",
    "   - `source venv/bin/activate`\n",
    "3. Install required python libraries from `ot_requirements.txt` file\n",
    "   - `pip install -r ot_requirements.txt`\n",
    "4. Select `venv` as python kernel for this JupyterNotebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b88d4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4ff7bc",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a90a109",
   "metadata": {},
   "outputs": [],
   "source": [
    "xr_ot = xr.open_dataset(\"data/mljc_workshop_o3_L25.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "098e1472",
   "metadata": {},
   "outputs": [],
   "source": [
    "ozone_arr = xr_ot[\"SpeciesConcVV_O3\"].data\n",
    "ozone_arr_trimmed = ozone_arr[:, :-2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c07523ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ozone_chunked = ozone_arr_trimmed.reshape(\n",
    "    ozone_arr_trimmed.shape[0], \n",
    "    ozone_arr_trimmed.shape[1] // 11, 11, \n",
    "    ozone_arr_trimmed.shape[2] // 12, 12\n",
    ").swapaxes(2, 3).swapaxes(0, 2)\n",
    "ozone_chunked = ozone_chunked.reshape(-1, *ozone_chunked.shape[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8738dc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ozone_chunked_trimmed = ozone_chunked[:, :-20, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0583fccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ozone_chunked = ozone_chunked_trimmed.reshape(\n",
    "    ozone_chunked_trimmed.shape[0], \n",
    "    ozone_chunked_trimmed.shape[1] // 100, 100, \n",
    "    ozone_chunked_trimmed.shape[2], \n",
    "    ozone_chunked_trimmed.shape[3]\n",
    ")\n",
    "ozone_chunked = ozone_chunked.reshape(-1, *ozone_chunked.shape[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b393539f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ozone_chunked = ozone_chunked.reshape(*ozone_chunked.shape[:-2], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b608999",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP_SHAPE = (11, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58af22f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(696, 100, 132)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ozone_chunked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca40225f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_tensor_right(x, shift, dim):\n",
    "    size = x.size(dim)\n",
    "    if shift >= size:\n",
    "        # Entire tensor becomes zero\n",
    "        return torch.zeros_like(x)\n",
    "\n",
    "    # Slice from start to size - shift along dim\n",
    "    sliced = x.narrow(dim, 0, size - shift)\n",
    "\n",
    "    # Pad with zeros at the beginning along that dimension\n",
    "    pad_shape = list(x.shape)\n",
    "    pad_shape[dim] = shift\n",
    "    pad_tensor = torch.zeros(pad_shape, dtype=x.dtype, device=x.device)\n",
    "\n",
    "    return torch.cat((pad_tensor, sliced), dim=dim)\n",
    "\n",
    "shift_sequence_dim_right = functools.partial(shift_tensor_right, shift=1, dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be486ddf",
   "metadata": {},
   "source": [
    "The dimension of the dataset: (batch_size, sequence_size, embedding_size)\n",
    "\n",
    "We have following dataset:\n",
    "- input: `ozone_chunked`\n",
    "- output: `ozone_chunked_shifted`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd86b2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_train, SRC_test = train_test_split(ozone_chunked, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f74c696",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_train = torch.tensor(SRC_train)\n",
    "SRC_test  = torch.tensor(SRC_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df9192d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TGT_train = shift_sequence_dim_right(SRC_train)\n",
    "TGT_test = shift_sequence_dim_right(SRC_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16be4216",
   "metadata": {},
   "source": [
    "Divided into train and test dataset (let's ignore validation datasets)\n",
    "- Train:\n",
    "    - `SRC_train`\n",
    "    - `TGT_train`\n",
    "- Test:\n",
    "    - `SRC_test`\n",
    "    - `TGT_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1eee026a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = TensorDataset(SRC_train, TGT_train)\n",
    "dataset_test = TensorDataset(SRC_test, TGT_test)\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=16, shuffle=True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dff766",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e294a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sinusoidal_positional_embedding(sequence_size, embedding_size):\n",
    "    pos = torch.arange(0, sequence_size, dtype=torch.float).unsqueeze(1)\n",
    "    i = torch.arange(0, embedding_size, 2).float()\n",
    "    angle_rates = 1 / (10000 ** (i / embedding_size))\n",
    "    \n",
    "    angle_rads = pos * angle_rates  # [seq_len, d_model/2]\n",
    "\n",
    "    pe = torch.zeros(sequence_size, embedding_size)\n",
    "    pe[:, 0::2] = torch.sin(angle_rads)\n",
    "    pe[:, 1::2] = torch.cos(angle_rads)\n",
    "\n",
    "    return pe  # [seq_len, d_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc0d6ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = SRC_test.shape[2]\n",
    "SEQUENCE_SIZE = SRC_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e6e7b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(torch.nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            embedding_size, \n",
    "            n_head, n_encoder_layer, n_decoder_layer, n_feedforward, \n",
    "            positional_embedding_func,\n",
    "            sequence_dim_shift_func,\n",
    "        ):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        self.positional_encoding_func = positional_embedding_func\n",
    "        self.sequence_dim_shift_func = sequence_dim_shift_func\n",
    "\n",
    "        self.transformer = torch.nn.Transformer(\n",
    "            embedding_size, \n",
    "            n_head, \n",
    "            n_encoder_layer, \n",
    "            n_decoder_layer, \n",
    "            n_feedforward, \n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x += self.positional_encoding_func(x.shape[1], x.shape[2])\n",
    "        x_shifted = self.sequence_dim_shift_func(x)\n",
    "        x = self.transformer(x, x_shifted)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a121e9",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93d8d669",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerModel(EMBEDDING_SIZE, 12, 6, 6, 2048, get_sinusoidal_positional_embedding, shift_sequence_dim_right)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c9b6a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of TransformerModel(\n",
      "  (transformer): Transformer(\n",
      "    (encoder): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=132, out_features=132, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=132, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=132, bias=True)\n",
      "          (norm1): LayerNorm((132,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((132,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((132,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (decoder): TransformerDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=132, out_features=132, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=132, out_features=132, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=132, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=132, bias=True)\n",
      "          (norm1): LayerNorm((132,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((132,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((132,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((132,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "print(model.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34b606ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([556, 100, 132])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SRC_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d840d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    # total_batches = 0\n",
    "    total_batches = len(train_loader)\n",
    "    \n",
    "    for texts, labels in tqdm(train_loader):\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(texts)\n",
    "        \n",
    "        # Compute loss and gradients\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        correct = (preds == labels).sum().item()\n",
    "        accuracy = correct / labels.size(0)\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_accuracy += accuracy\n",
    "    \n",
    "    return epoch_loss / total_batches, epoch_accuracy / total_batches\n",
    "\n",
    "def evaluate(model, test_loader, loss_fn):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    total_batches = len(test_loader)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for texts, labels in tqdm(test_loader):\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            # Forward pass\n",
    "            outputs = model(texts)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct = (preds == labels).sum().item()\n",
    "            accuracy = correct / labels.size(0)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_accuracy += accuracy\n",
    "    \n",
    "    return epoch_loss / total_batches, epoch_accuracy / total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57add7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ab88c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdecdbe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2b4e10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b7907e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
